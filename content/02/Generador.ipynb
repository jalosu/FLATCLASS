{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edacb949-7f43-44dd-b752-90d9bebb70df",
   "metadata": {},
   "source": [
    "# Generación de datos sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a934fd-b53d-4afb-929e-e97dfa1aadf4",
   "metadata": {},
   "source": [
    "````{admonition} Resumen \n",
    ":class: tip\n",
    "\n",
    "Este artículo presenta las líneas de trabajo desarrolladas para la generación de un dataset sintético de tamaño y peso de lenguados, con el objetivo de proporcionar una base de datos suficientemente amplia y representativa para el entrenamiento, validación y mejora de modelos predictivos. Se recogen las metodologías utilizadas en la generación de datos sintéticos, incluyendo la modelización estadística de distribuciones empíricas, técnicas de simulación basadas en procesos de crecimiento biológico y enfoques de aprendizaje automático para la síntesis de datos realistas. Además, se analizan los criterios de validación empleados para garantizar que los datos generados reflejen fielmente las tendencias y variabilidad observadas en poblaciones reales de lenguado (*Solea solea*), asegurando así su utilidad en el desarrollo de algoritmos precisos y generalizables para la predicción del peso a partir de variables alometricas.\n",
    "\n",
    "**Entregable**: E2.2  \n",
    "**Versión**: 1.0  \n",
    "**Autor**: Javier Álvarez Osuna  \n",
    "**Email**: javier.osuna@fishfarmfeeder.com  \n",
    "**ORCID**: [0000-0001-7063-1279](https://orcid.org/0000-0001-7063-1279)  \n",
    "**Licencia**: CC-BY-4.0  \n",
    "**Código proyecto**: IG408M.2025.000.000072\n",
    "\n",
    "```{figure} .././assets/FLATCLASS_logo_publicidad.png\n",
    ":width: 100%\n",
    ":align: center\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44580fd0-2d35-4fc0-9084-d562d01b4003",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En el ámbito de la acuicultura de precisión, la caracterización morfométrica de los peces y su relación con el peso corporal constituye un eje central para la optimización de procesos como la clasificación automática, el control de crecimiento y la dosificación alimentaria. En el caso particular de los peces planos en fase de alevinaje —como el lenguado (Solea solea) o el rodaballo (Scophthalmus maximus)—, las variables morfométricas fundamentales incluyen la longitud corporal, la anchura transversal y la altura dorso-ventral, parámetros que definen la geometría del individuo y que se presumen relacionados de forma sistemática con la biomasa individual.\n",
    "\n",
    "La necesidad de disponer de un dataset suficientemente amplio, representativo y multivariado, que relacione estas variables morfométricas con el peso corporal correspondiente, responde a múltiples consideraciones de carácter estadístico, biológico y computacional. Aun en ausencia de un modelo alométrico explícito que relacione de forma determinista dichas variables, es posible anticipar que cualquier estrategia de inferencia o predicción del peso basada en dimensiones requerirá una densidad adecuada de datos en el espacio tridimensional definido por longitud, anchura y altura. Este requisito es crítico para garantizar tanto la fidelidad del ajuste como la capacidad de generalización del modelo aprendido.\n",
    "\n",
    "Cuando el volumen de datos disponibles es reducido, surgen una serie de limitaciones estructurales:\n",
    "\n",
    "- **Alta varianza en la estimación de parámetros**: La precisión de los modelos predictivos decae significativamente cuando las observaciones son escasas o están mal distribuidas en el dominio de entrada.\n",
    "- **Riesgo de sobreajuste**: En entornos de datos reducidos, los modelos tienden a capturar ruido en lugar de relaciones funcionales genuinas, lo cual compromete la validez externa.\n",
    "- **Cobertura insuficiente del espacio morfométrico**: Se produce una pérdida de representatividad en las regiones marginales del dominio, lo que reduce la capacidad del sistema para extrapolar o interpolar en condiciones reales de producción.\n",
    "- **Sesgos estructurales**: Las muestras pequeñas pueden reflejar sesgos en las condiciones de cría, genética o instrumentación, induciendo patrones espurios no generalizables.\n",
    "\n",
    "Desde la perspectiva de la matemática probabilística, esta problemática puede entenderse mediante el marco de la inferencia bayesiana. En este enfoque, el conocimiento sobre los parámetros $\\theta$ (por ejemplo, la relación entre morfología y peso) se representa como una distribución posterior condicionada a los datos $D$:\n",
    "\n",
    "$$\n",
    "p(\\theta | D) \\propto p(D | \\theta) \\cdot p(\\theta)\n",
    "$$\n",
    "\n",
    "donde $D$ representa los datos observados. Cuando el tamaño de $D$ es reducido, la función de verosimilitud $p(D|a,b)$ tiene una varianza alta, lo que genera estimaciones más inciertas. Al aumentar el tamaño de $D$ con datos sintéticos plausibles, la estimación de $p(a,b|D)$ se vuelve más precisa, reduciendo la varianza de los parámetros. \n",
    "\n",
    "En este trabajo se ha empleado una arquitectura de **Redes Generativas Adversarias (GAN, por sus siglas en inglés)** como estrategia para la **generación de datos sintéticos tabulares** que relacionan variables morfométricas de juveniles de peces planos (longitud, anchura y altura) con el peso corporal correspondiente. Las GANs, originalmente concebidas para tareas de síntesis de imágenes [[Goodfellow et al., 2014](https://doi.org/10.1145/3422622)], han demostrado en los últimos años una notable capacidad para modelar distribuciones complejas en espacios tabulares multivariantes, especialmente en dominios donde los datos reales son escasos o costosos de obtener.\n",
    "\n",
    "En el contexto de la acuicultura, la recopilación masiva de medidas biométricas de precisión en alevines presenta limitaciones logísticas y económicas significativas. Por tanto, el uso de GANs permite simular observaciones adicionales coherentes con la distribución empírica observada, conservando las correlaciones entre las dimensiones corporales y el peso de los individuos. En este estudio se ha diseñado una GAN compuesta por dos redes neuronales multicapas —un generador y un discriminador— entrenadas adversarialmente sobre un conjunto limitado de datos reales, lo que permite sintetizar de forma controlada nuevos registros morfométricos plausibles y estadísticamente robustos.\n",
    "\n",
    "Diversos estudios recientes avalan el uso de GANs para la generación de datos tabulares continuos en contextos biológicos y clínicos. Por ejemplo, [Xu et al. (2023)](https://doi.org/10.1007/s00778-023-00807-y) demuestran que las GANs superan a técnicas convencionales de síntesis en la preservación de estructuras de dependencia no lineal en variables tabulares reales de alta dimensión. Asimismo, [Lin et al. (2023)](https://doi.org/10.3389/fdata.2023.1296508) introducen **CTAB-GAN+**, una variante especial que mejora la fidelidad y utilidad de los datos generados en dominios tabulares complejos, destacando su aplicabilidad en entornos de escasez de datos\n",
    "\n",
    "El uso de GANs en este trabajo responde, por tanto, a una doble motivación: por un lado, **ampliar artificialmente el conjunto de datos disponible** para entrenar modelos predictivos del peso a partir de variables morfométricas; y por otro, **mantener la coherencia estadística y biológica** de los registros generados, minimizando los riesgos de sobreajuste y mejorando la capacidad de generalización de los modelos desarrollados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ca3be-7f77-412f-9f03-be5872a88d85",
   "metadata": {},
   "source": [
    "## Fundamento teórico\n",
    "\n",
    "Las redes antagónicas generativas o redes adversarias generativas (GANs) son un método para la optimización competitivo entre dos redes neuronales, una llamada generadora y otra discriminadora, con el objetivo de conseguir generar nuevas instancias idealmente indistinguibles a las pertenecientes a la distribución de probabilidad de la que derivan los datos de entrenamiento.\n",
    "\n",
    "El fundamento teórico general del que derivan, permite su utilización para la generación de cualquier tipo de datos, habiéndose demostrado efectiva en campos diversos como son la visión por computador [[Roy et al., 2015](https://doi.org/10.48550/arXiv.1505.03906), [Karras et al., 2017](https://doi.org/10.48550/arXiv.1710.10196)], la segmentación semántica [[Hoffman et al., 2017](https://doi.org/10.48550/arXiv.1711.03213)], la síntesis de series temporales [[Hartmann et al., 2018](https://doi.org/10.48550/arXiv.1806.01875)], la edición de imagen [[Abdal et al., 2020](\n",
    "https://doi.org/10.1145/3447648)], el procesamiento del lenguaje natural [[Fedus et al., 2018](https://doi.org/10.48550/arXiv.1801.07736)], la generación de imagen a partir de texto [[Radford et al., 2021](\n",
    "https://doi.org/10.48550/arXiv.2103.00020)] y más recientemente en la generación de datos tabulares complejos [[Lin et al. 2023](https://doi.org/10.3389/fdata.2023.1296508)].\n",
    "\n",
    "Para cualquier conjunto de datos, podemos hipotetizar que es posible definir una distribución de probabilidad $P_{data}$ representativa de la población representada por la muestra formada por el conjunto de datos. De ser esto posible, para cualquier valor de $x$ será posible establecer un valor $P_{data}(x)$ que determine la probabilidad de que $x$ pertenezca a la población. De existir una función de este tipo, sería una función discriminativa que dada una instancia permitiría conocer la probabilidad de pertenencia a la población. Los modelos generativos modelizan la distribución de probabilidad mencionada pero no proporcionan un valor de probabilidad, sino que generan instancias nuevas que pertenecen a distribuciones de probabilidad próximas a la que pretenden asemejar. Las GANs definen un esquema de aprendizaje que facilita la codificación de los atributos definitorios de la distribución de probabilidad en una red neuronal de manera que la red incorpore la información esencial que le permite generar instancias pertenecientes a distribuciones de probabilidad próximas a la que el conjunto de datos que pretende representar.\n",
    "\n",
    "La arquitectura de las Redes Generativas Adversarias (GAN, por sus siglas en inglés) se basa en la interacción entre dos redes neuronales que trabajan de forma opuesta pero complementaria: una red generadora $(G)$ y una red discriminadora $(D)$. La red generadora tiene como objetivo crear datos sintéticos que imiten con la mayor fidelidad posible los datos reales del conjunto original. Por su parte, la red discriminadora actúa como un detector, cuya tarea es evaluar si una determinada entrada procede del conjunto de datos reales o ha sido generada artificialmente por $G$.\n",
    "\n",
    "Durante el proceso de entrenamiento, ambas redes se enfrentan en un proceso competitivo. La generadora intenta “engañar” a la discriminadora creando datos cada vez más realistas, mientras que la discriminadora se entrena para detectar con mayor precisión las falsificaciones. Este enfoque adversarial permite que ambas redes mejoren progresivamente: $G$ produce datos sintéticos más convincentes y $D$ refina sus capacidades de detección. Esta dinámica se puede entender como un juego de suma cero, donde el éxito de una red implica el fracaso de la otra, y que teóricamente puede llevar a un punto en el que ninguna de las redes puede mejorar su rendimiento sin afectar negativamente a la otra (equilibrio de Nash).\n",
    "\n",
    "```{figure} .././assets/Modelo_GAN.png\n",
    ":name: Figura_3.1\n",
    ":alt: Diagrama del proceso de entrenamiento de las redes adversarias generativas\n",
    ":width: 100%\n",
    ":align: center\n",
    "```\n",
    "\n",
    "En la figura se muestra un diagrama representativo del proceso de optimización de las GAN. La **red generadora** $G(z)$ recibe como entrada un vector de ruido aleatorio $z$, generado a partir de una distribución conocida $p_z$, y produce como salida un dato sintético $x_{fake}$ que intenta imitar los datos reales. La **red discriminadora** $D(x)$ recibe como entrada un dato $x_{real}$  o $x_{fake}$ (generado) y devuelve una probabilidad `D(x)` entre $0$ y $1$ que indica cuán probable es que $x$ provenga del conjunto real de datos.\n",
    "\n",
    "Amabas redes se consideran antagónicas dado que sus objetivos son opuestos:\n",
    "\n",
    "- $D$ quiere **maximizar** la probabilidad de detectar correctamente los datos reales y rechazar los sintéticos.\n",
    "- $G$ quiere **minimizar** la probabilidad de que $D$ detecte que sus datos son falsos.\n",
    "\n",
    "El proceso se modela como un **juego de suma cero** mediante la siguiente función objetivo:\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{datos}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "- El primer término recompensa a $D$ por identificar correctamente datos reales.\n",
    "- El segundo término recompensa a $D$ por detectar correctamente los datos generados por $G$.\n",
    "\n",
    "Mientras tanto, $G$ intenta minimizar esta función engañando a $D$, es decir, haciendo que $D(G(z))$ sea lo más cercano posible a 1.\n",
    "\n",
    "Durante el entrenamiento, ambas redes mejoran iterativamente. Teóricamente, el proceso puede converger a un **equilibrio de Nash**, en el que $G$ genera datos tan similares a los reales que $D$ no puede distinguir entre ellos, y devuelve aproximadamente:\n",
    "\n",
    "$$\n",
    "D(x) \\approx 0.5\n",
    "$$\n",
    "\n",
    "En ese punto, el sistema ha alcanzado un equilibrio: ni $G$ ni $D$ pueden mejorar sin perjudicar a la otra red."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a9a50-fa21-4844-a5a5-d3d48a2f2498",
   "metadata": {},
   "source": [
    "## Algoritmo generador de datos morfométricos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5480bc-ab63-4a43-ac5f-1c4bfa7ee9c5",
   "metadata": {},
   "source": [
    "Este algoritmo entrena una Red Generativa Adversa (GAN) usando `TensorFlow`para generar $N$ registros sintéticos de longitud, anchura, altura y peso de juveniles de lenguado, usando como punto de partida el dataset obtenido en condiciones reales de medición única ([**Obtención y etiquetado del Dataset-E2.1**](./Dataset.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e36526-c30a-46d9-a1fb-0bd1677ed554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fedc055-6862-44f2-accf-4ba75c453114",
   "metadata": {},
   "source": [
    "### Carga y procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6f5c6a-c4a2-46bf-81b3-8a4d689cbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "df = pd.read_excel(\".././data/Dimensiones_lenguado.xlsx\", sheet_name=\"Hoja1\")\n",
    "\n",
    "# Selección de variables relevantes\n",
    "data = df[['Longitud (cm)', 'Anchura (cm)', 'Altura (cm)', 'Peso (g)']].values\n",
    "\n",
    "# Escalado [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d46ac43-d2f7-4fde-b008-32c1beb611bf",
   "metadata": {},
   "source": [
    "Las GANs son especialmente sensibles a desequilibrios de escala entre las variables de entrada y salida. Sin una normalización adecuada, las activaciones internas de las capas neuronales pueden caer en regiones no óptimas de las funciones de activación, dificultando la retropropagación del gradiente. Al restringir los datos al rango $[0, 1]$, se asegura que la salida del generador y la entrada del discriminador se mantengan dentro de un dominio numérico estable, lo cual:\n",
    " - Facilita la convergencia del entrenamiento.\n",
    " - Evita gradientes explosivos o desvanecidos.\n",
    " - Mejora la capacidad del generador para aproximar la distribución objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e57669-a4df-49aa-b6ac-45865a004ffe",
   "metadata": {},
   "source": [
    "### Definición de la arquitectura GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bfa8278-054f-412b-bc11-6e2c4610c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#      ARQUITECTURA GAN\n",
    "###############################\n",
    "\n",
    "latent_dim = 32\n",
    "\n",
    "# Generador\n",
    "def build_generator():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation=\"relu\", input_dim=latent_dim),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(4, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminador\n",
    "def build_discriminator():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, input_shape=(4,), activation=\"leaky_relu\"),\n",
    "        layers.Dense(64, activation=\"leaky_relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d0236-e2b9-4be7-afbd-804f76d795b6",
   "metadata": {},
   "source": [
    "El código define la arquitectura básica de una Red Generativa Adversaria (GAN) compuesta por dos redes neuronales artificiales: el generador (`build_generator()`) y el discriminador (`build_discriminator`), ambas implementadas mediante modelos secuenciales.\n",
    "\n",
    "El generador tiene como entrada un vector de ruido aleatorio de dimensión 32 (`latent_dim = 32`). Este vector es transformado a través de dos capas densas ocultas con 64 neuronas cada una y función de activación `ReLU`, seguidas por una capa de salida con 4 neuronas y activación sigmoide, cuya función es producir una muestra sintética normalizada de cuatro variables continuas (en este caso: longitud, anchura, altura y peso de un pez). Por su parte, el discriminador recibe como entrada una muestra de 4 variables —ya sea real o generada— y la procesa mediante dos capas densas de 64 neuronas con activación `LeakyReLU`, seguida de una capa de salida con una única neurona y activación sigmoide que devuelve una probabilidad de autenticidad.\n",
    "\n",
    "En ambos casos se ha usado una **función de activación sigmoide**. Èsta es una función matemática del tipo:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "cuya salida se restringe al rango $[0, 1]$. Esta función es la elección idónea cuando se requiere interpretar la salida de una neurona como una probabilidad o como una variable continua normalizada. En el caso del generador definido en el código, la activación sigmoide se aplica en la capa de salida para garantizar que cada una de las cuatro variables generadas (longitud, anchura, altura y peso) esté contenida dentro del rango $[0, 1]$, en coherencia con el preprocesamiento previo de los datos reales mediante normalización `MinMaxScaler()`. Esto permite una comparación justa y estable entre las muestras sintéticas y las reales durante el entrenamiento competitivo. Asimismo, en el discriminador, la activación sigmoide en la salida final se justifica porque su tarea es emitir una probabilidad de veracidad —esto es, si una muestra es real (próxima a 1) o generada (próxima a 0)—, lo que hace que la sigmoide sea la elección natural para tareas de clasificación binaria dentro del marco de aprendizaje profundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f1fbc-fbd9-4687-b530-c0e884bc00cf",
   "metadata": {},
   "source": [
    "La **función de activación `ReLU` (Rectified Linear Unit)** es una de las funciones no lineales más utilizadas en redes neuronales profundas debido a su simplicidad y eficacia computacional. Se define como $\\text{ReLU}(x) = \\max(0, x)$, lo que implica que las salidas negativas se eliminan (se convierten en cero) mientras que las positivas se mantienen sin alteración. Esta propiedad introduce no linealidad en la red neuronal, permite que las neuronas se activen solo cuando es necesario y, al mismo tiempo, evita problemas de saturación que se presentan en otras funciones. Sin embargo, `ReLU` puede presentar el problema del “apagado de neuronas” (dead neurons), cuando los valores de entrada son negativos de forma persistente, impidiendo que esas neuronas contribuyan al aprendizaje.\n",
    "\n",
    "Para mitigar este problema, se utiliza la **función de activación `Leaky ReLU`**, una variante que introduce una pequeña pendiente negativa para los valores menores que cero. Su definición es:\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{si } x \\geq 0 \\\\\n",
    "\\alpha x & \\text{si } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "donde $\\alpha$ es un pequeño valor positivo, típicamente $0.01$. Esta modificación permite que las neuronas continúen actualizando sus pesos incluso cuando sus entradas son negativas, lo que favorece una mejor propagación del gradiente durante el entrenamiento y mejora la robustez de la red.\n",
    "\n",
    "En el código anterior, se ha optado por emplear `ReLU` en el generador para promover una activación fuerte y directa a partir del espacio latente, mientras que en el discriminador se utiliza `LeakyReLU` para evitar el colapso de unidades activas y asegurar una mejor capacidad de detección de patrones tanto en regiones positivas como negativas del espacio de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239886a-f5fe-41b2-9943-6281efcf3415",
   "metadata": {},
   "source": [
    "Por último se ha optadoo por el uso de **64 neuronas en las capas ocultas** tanto del generador como del discriminador ya que responde a un compromiso técnico entre capacidad representacional, estabilidad del entrenamiento y eficiencia computacional, especialmente en el contexto de modelos generativos aplicados a datos tabulares de baja dimensionalidad, como es el caso de las variables morfométricas (longitud, anchura, altura y peso) de juveniles de peces.\n",
    "\n",
    "Desde un punto de vista práctico, 64 unidades por capa ofrecen una capacidad suficiente para capturar relaciones no lineales complejas entre las variables del espacio latente y las características morfológicas de salida, sin llegar a una sobreparametrización excesiva que pueda inducir sobreajuste o inestabilidad adversarial durante el entrenamiento de la GAN. Esta elección proporciona un número razonable de parámetros entrenables, lo cual resulta adecuado cuando se trabaja con conjuntos de datos de tamaño moderado, como ocurre en nuestro caso.\n",
    "\n",
    "Además, estudios recientes sobre GANs para datos tabulares —como CTAB-GAN+ [[Zao et al., 2024](https://doi.org/10.3389/fdata.2023.1296508)] y Tabular GAN [[Liu et al., 2023](https://doi.org/10.1007/s00778-023-00807-y)]— concluyen que arquitecturas con capas ocultas de entre 64 y 128 neuronas suelen alcanzar un buen equilibrio entre precisión, velocidad de convergencia y estabilidad del discriminador. Por debajo de este umbral (p.e., 16 o 32 neuronas), se puede observar pérdida de capacidad expresiva, mientras que valores superiores (p.e., 256) pueden ser innecesarios para problemas con pocos atributos y generar ruido o fluctuaciones en el aprendizaje adversarial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37900c62-bdb2-4117-b1b2-b5f084bec444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
