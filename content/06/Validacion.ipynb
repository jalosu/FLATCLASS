{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e694732b",
   "metadata": {},
   "source": [
    "# Validaci√≥n en entorno industrial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4f672-f347-4ad4-8ee6-905ed1c62bfa",
   "metadata": {},
   "source": [
    "````{admonition} Resumen \n",
    ":class: tip\n",
    "\n",
    "Este informe presenta la validaci√≥n en entorno productivo real del sistema FLATCLASS, el prototipo autom√°tico de clasificaci√≥n de alevines de lenguado (*Solea senegalensis*) basado en visi√≥n artificial. La validaci√≥n se realiz√≥ en las instalaciones de Satistela S.A. ([Grupo Sea8](https://https://seaeight.eu/contacto/)) en P√≥voa de Varzim (Portugal), mediante un protocolo multi-experimento dise√±ado para cuantificar: 1) la precisi√≥n en la clasificaci√≥n por intervalos de talla (peque√±o, mediano y grande), y 2) la exactitud en la estimaci√≥n individual de biomasa de cada individuo detectado. El estudio se apoya en 1.250 observaciones (5 experimentos, 250 peces) y eval√∫a: exactitud, precisi√≥n, estabilidad inter-experimento, acuerdo metrol√≥gico (Bland‚ÄìAltman), heterocedasticidad y un an√°lisis Monte Carlo del riesgo de error de clasificaci√≥n inducido por la incertidumbre dimensional.\n",
    "\n",
    "**Entregable**: E6.1  \n",
    "**Versi√≥n**: 1.0  \n",
    "**Autor**: Javier √Ålvarez Osuna  \n",
    "**Email**: javier.osuna@fishfarmfeeder.com  \n",
    "**ORCID**: [0000-0001-7063-1279](https://orcid.org/0000-0001-7063-1279)  \n",
    "**Licencia**: CC-BY-4.0  \n",
    "**C√≥digo proyecto**: IG408M.2025.000.000072\n",
    "\n",
    "```{figure} .././assets/FLATCLASS_logo_publicidad.png\n",
    ":width: 100%\n",
    ":align: center\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b495083-b857-42ce-95fa-d3a261b10d4e",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "El **objetivo general** planteado en la validaci√≥n industrial de FLATCLASS reside en comprobar hasta qu√© punto el sistema de visi√≥n artificial desarrollado en el {doc}`PT-1 <content/01/Modulo-3>` es capaz de clasificar de forma fiable los juveniles de lenguado en las tres diferentes tallas (peque√±o, normal, grande), teniendo en cuenta la incertidumbre inherente a las mediciones realizadas en condiciones reales de operaci√≥n.\n",
    "\n",
    "Para ello, se han planteado los siguientes **objetivos espec√≠ficos** orientados a analizar, de manera estructurada y cuantitativa, los distintos factores que condicionan la fiabilidad del proceso de clasificaci√≥n. Estos objetivos permiten descomponer el problema global en aspectos medibles relacionados con la calidad metrol√≥gica de las estimaciones dimensionales, su estabilidad temporal, el grado de acuerdo con las medidas de referencia y el impacto pr√°ctico de los errores de medici√≥n sobre la decisi√≥n final de asignaci√≥n de talla.\n",
    "\n",
    " 1. Caracterizar estad√≠sticamente la exactitud y precisi√≥n de las mediciones dimensionales (longitud y anchura) obtenidas mediante el sistema de visi√≥n artificial, compar√°ndolas con las medidas de referencia realizadas en laboratorio, con el fin de cuantificar el error medio, la dispersi√≥n y la presencia de posibles sesgos sistem√°ticos.\n",
    " 2.\tEvaluar el grado de acuerdo metrol√≥gico entre las medidas estimadas y las reales, utilizando an√°lisis de Bland‚ÄìAltman y m√©tricas complementarias, para interpretar la incertidumbre del sistema directamente en unidades f√≠sicas relevantes para la clasificaci√≥n por talla.\n",
    " 3.\tAnalizar la estabilidad temporal del sistema de medici√≥n y clasificaci√≥n, comparando el comportamiento estad√≠stico de los errores a lo largo de varios experimentos consecutivos realizados en d√≠as distintos, con el objetivo de detectar posibles derivas o variaciones operativas.\n",
    " 4.\tExaminar la relaci√≥n entre el tama√±o del individuo y la magnitud del error de medida, identificando posibles efectos de heterocedasticidad y dependencias con la talla que puedan influir en la fiabilidad de la clasificaci√≥n en distintos rangos de tama√±o.\n",
    " 5.\tEvaluar el desempe√±o del peso inferido como variable secundaria, analizando su precisi√≥n y variabilidad, y determinar su idoneidad para la estimaci√≥n de biomasa agregada por clase de talla, en lugar de su uso como criterio primario de clasificaci√≥n individual.\n",
    " 6.\tCuantificar el impacto pr√°ctico de la incertidumbre dimensional sobre la decisi√≥n de clasificaci√≥n por talla, mediante simulaciones Monte Carlo que permitan estimar la probabilidad de cambio de clase inducida por los errores de medici√≥n.\n",
    " 7.\tIdentificar los rangos de talla m√°s sensibles a la inestabilidad de clasificaci√≥n, especialmente en torno a los umbrales entre clases, con el fin de caracterizar las zonas de mayor riesgo de asignaci√≥n incorrecta.\n",
    " 8.\tProponer criterios estad√≠sticos para el dise√±o y validaci√≥n de los umbrales de clasificaci√≥n por talla, basados en percentiles del error dimensional, que permitan reducir el riesgo de inestabilidad de clasificaci√≥n en aplicaciones operativas.\n",
    " 9.\tProporcionar una base metodol√≥gica reproducible para la validaci√≥n de sistemas de clasificaci√≥n por visi√≥n artificial, integrando an√°lisis metrol√≥gico, estad√≠stico y de toma de decisiones, aplicable a otros contextos industriales y especies acu√≠colas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c0c79-5629-4860-ab7a-26ab3245b44b",
   "metadata": {},
   "source": [
    "## Metodolog√≠a\n",
    "\n",
    "### Dise√±o experimental\n",
    "\n",
    "El dataset contiene **1.250 registros** correspondientes a **cinco experimentos realizados en d√≠as consecutivos** (`exp = 1..5`), con **250 juveniles por experimento**. Cada individuo se mide en cinta mediante un sistema \n",
    "de visi√≥n artificial que entrega dimensiones de **longitud** y **anchura** en p√≠xeles que mediante conversi√≥n calibrada se registran en mil√≠metros (`L_av`, `A_av`). A partir de estas dimensiones se produce la inferencia de peso en tiempo real (`peso_pred`)mediante el modelo alom√©trico previamente validado (PT-3). Como referencia, se registra **longitud real** (`L_real`), **anchura real** (`A_real`) mediante tallado manual en medici√≥n √∫nica y **peso real** (`peso_g`) mediante b√°scula.\n",
    "\n",
    "Las condiciones experimentales (c√°mara, iluminaci√≥n, calibraci√≥n geom√©trica, procedimiento) fueron id√©nticas en los cinco d√≠as, permitiendo evaluar la estabilidad temporal del sistema.\n",
    "\n",
    "```{admonition} Objetivo del sistema\n",
    ":class: important\n",
    "El objetivo operativo principal es la **clasificaci√≥n por talla** en tres categor√≠as discretas (peque√±o, mediano, grande) basada principalmente en las **dimensiones**.  \n",
    "El peso inferido se considera objetivo secundario para **biomasa agregada por clase**.\n",
    "```\n",
    "\n",
    "### Variables disponibles en el dataset\n",
    "\n",
    "El an√°lisis utiliza **exclusivamente** las siguientes columnas, sin generar variables adicionales no documentadas:\n",
    "\n",
    "| Variable | Columna |\n",
    "|--------|--------|\n",
    "| Experimento / d√≠a | `exp` |\n",
    "| Peso real (g) | `peso_g` |\n",
    "| Longitud real (mm) | `L_real` |\n",
    "| Anchura real (mm) | `A_real` |\n",
    "| Longitud en p√≠xeles | `Longitud_px` |\n",
    "| Anchura en p√≠xeles | `Anchura_px` |\n",
    "| Longitud estimada por visi√≥n (mm) | `L_av` |\n",
    "| Anchura estimada por visi√≥n (mm) | `A_av` |\n",
    "| Error relativo longitud (%) | `errL_pct` |\n",
    "| Error relativo anchura (%) | `errA_pct` |\n",
    "| Peso inferido (g) | `peso_pred` |\n",
    "| Error absoluto peso (g) | `err_abs_peso` |\n",
    "| Error relativo peso (%) | `err_rel_peso` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c217cfc-8227-4d7a-98ac-fca99ad19590",
   "metadata": {},
   "source": [
    "## Carga y validaci√≥n inicial del dataset\n",
    "\n",
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade61186-361e-4fba-a3a0-d56c3cd27bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>peso_g</th>\n",
       "      <th>L_real</th>\n",
       "      <th>A_real</th>\n",
       "      <th>Longitud_px</th>\n",
       "      <th>Anchura_px</th>\n",
       "      <th>L_av</th>\n",
       "      <th>A_av</th>\n",
       "      <th>errL_pct</th>\n",
       "      <th>errA_pct</th>\n",
       "      <th>peso_pred</th>\n",
       "      <th>err_abs_peso</th>\n",
       "      <th>err_rel_peso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>302</td>\n",
       "      <td>118</td>\n",
       "      <td>33.30</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.483591</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>5.128430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.46</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>299</td>\n",
       "      <td>118</td>\n",
       "      <td>32.96</td>\n",
       "      <td>13.01</td>\n",
       "      <td>-0.121212</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.473780</td>\n",
       "      <td>0.013780</td>\n",
       "      <td>2.995702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>299</td>\n",
       "      <td>118</td>\n",
       "      <td>32.96</td>\n",
       "      <td>13.01</td>\n",
       "      <td>-0.121212</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.473780</td>\n",
       "      <td>0.013780</td>\n",
       "      <td>2.995702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.67</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>351</td>\n",
       "      <td>136</td>\n",
       "      <td>38.70</td>\n",
       "      <td>14.99</td>\n",
       "      <td>-0.769231</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.748872</td>\n",
       "      <td>0.078872</td>\n",
       "      <td>11.771917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>41</td>\n",
       "      <td>17</td>\n",
       "      <td>375</td>\n",
       "      <td>154</td>\n",
       "      <td>41.34</td>\n",
       "      <td>16.98</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>-0.117647</td>\n",
       "      <td>0.963993</td>\n",
       "      <td>0.143993</td>\n",
       "      <td>17.560066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exp  peso_g  L_real  A_real  Longitud_px  Anchura_px   L_av   A_av  \\\n",
       "0    1    0.46      33      13          302         118  33.30  13.01   \n",
       "1    5    0.46      33      13          299         118  32.96  13.01   \n",
       "2    3    0.46      33      13          299         118  32.96  13.01   \n",
       "3    3    0.67      39      15          351         136  38.70  14.99   \n",
       "4    1    0.82      41      17          375         154  41.34  16.98   \n",
       "\n",
       "   errL_pct  errA_pct  peso_pred  err_abs_peso  err_rel_peso  \n",
       "0  0.909091  0.076923   0.483591      0.023591      5.128430  \n",
       "1 -0.121212  0.076923   0.473780      0.013780      2.995702  \n",
       "2 -0.121212  0.076923   0.473780      0.013780      2.995702  \n",
       "3 -0.769231 -0.066667   0.748872      0.078872     11.771917  \n",
       "4  0.829268 -0.117647   0.963993      0.143993     17.560066  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Imports y configuraci√≥n\n",
    "# =========================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# --- Rutas de entrada/salida (adaptadas para Jupyter-Book) ---\n",
    "DATA_PATH = \"../data/Dataset_validacion_final.xlsx\"\n",
    "\n",
    "# =========================\n",
    "# 1) Carga y preprocesado\n",
    "# =========================\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "# Renombrado\n",
    "rename_map = {\n",
    "    \"Experimento\": \"exp\",\n",
    "    \"Longitud_av\": \"L_av\",\n",
    "    \"Anchura_av\": \"A_av\",\n",
    "    \"err_rel_Longitud\": \"errL_pct\",\n",
    "    \"err_rel_Anchura\": \"errA_pct\",\n",
    "}\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "required = [\"exp\",\"peso_g\",\"L_real\",\"A_real\",\"Longitud_px\",\"Anchura_px\",\"L_av\",\"A_av\",\n",
    "            \"errL_pct\",\"errA_pct\",\"peso_pred\",\"err_abs_peso\",\"err_rel_peso\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas requeridas: {missing}\")\n",
    "df = df[required].copy()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6fd4b-b069-4faa-97ce-a28f9989a39e",
   "metadata": {},
   "source": [
    "### Verificaci√≥n de la escala de errores relativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aff24463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificaci√≥n de escala (errores relativos):\n",
      " - errL_pct: consistente con porcentaje (mediana |x - pct|=0).\n",
      " - errA_pct: consistente con porcentaje (mediana |x - pct|=0).\n",
      " - err_rel_peso: consistente con porcentaje (mediana |x - pct|=3.55e-15).\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 2) Validaci√≥n de escala de errores relativos\n",
    "# ----------------------------\n",
    "# Verificaci√≥n: err_% debe coincidir con (estimado-real)/real*100\n",
    "pct_L = (df[\"L_av\"] - df[\"L_real\"]) / df[\"L_real\"] * 100.0\n",
    "pct_A = (df[\"A_av\"] - df[\"A_real\"]) / df[\"A_real\"] * 100.0\n",
    "pct_P = (df[\"peso_pred\"] - df[\"peso_g\"]) / df[\"peso_g\"] * 100.0\n",
    "\n",
    "scale_notes = []\n",
    "for col, pct in [(\"errL_pct\", pct_L), (\"errA_pct\", pct_A), (\"err_rel_peso\", pct_P)]:\n",
    "    x = df[col].astype(float).to_numpy()\n",
    "    p = pct.astype(float).to_numpy()\n",
    "    mask = np.isfinite(x) & np.isfinite(p)\n",
    "    med_abs_direct = np.median(np.abs(x[mask] - p[mask]))\n",
    "    med_abs_x100 = np.median(np.abs(x[mask]*100.0 - p[mask]))\n",
    "    if med_abs_direct <= med_abs_x100:\n",
    "        scale_notes.append(f\"{col}: consistente con porcentaje (mediana |x - pct|={med_abs_direct:.3g}).\")\n",
    "    else:\n",
    "        df[col] = df[col] * 100.0\n",
    "        scale_notes.append(f\"{col}: parec√≠a fracci√≥n; convertido a porcentaje (√ó100).\")\n",
    "\n",
    "print(\"Verificaci√≥n de escala (errores relativos):\")\n",
    "for s in scale_notes:\n",
    "    print(\" -\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b69ad8-dbe7-4726-a390-104bb0baee14",
   "metadata": {},
   "source": [
    "## Control de calidad y tratamiento de *outliers*\n",
    "\n",
    "### Detecci√≥n robusta de valores at√≠picos\n",
    "\n",
    "Dado el car√°cter metrol√≥gico del estudio, se emplea un criterio robusto basado en z-score mediante MAD (Median Absolute Deviation) sobre:\n",
    " - error absoluto de longitud,\n",
    " - error absoluto de anchura,\n",
    " - error absoluto de peso.\n",
    "\n",
    "Se marca como outlier cualquier observaci√≥n con $‚à£ùëß_{MAD}|>5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c8cedf6-8a86-41dc-8ed4-0cd3ac9ddd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1250, 20), (1235, 20))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mad_z(x):\n",
    "    med = np.median(x)\n",
    "    mad = np.median(np.abs(x - med))\n",
    "    return 0.6745 * (x - med) / mad if mad > 0 else np.zeros_like(x)\n",
    "\n",
    "df[\"err_abs_L\"] = df[\"L_av\"] - df[\"L_real\"]\n",
    "df[\"err_abs_A\"] = df[\"A_av\"] - df[\"A_real\"]\n",
    "df[\"err_abs_P\"] = df[\"peso_pred\"] - df[\"peso_g\"]\n",
    "\n",
    "df[\"z_L\"] = mad_z(df[\"err_abs_L\"])\n",
    "df[\"z_A\"] = mad_z(df[\"err_abs_A\"])\n",
    "df[\"z_P\"] = mad_z(df[\"err_abs_P\"])\n",
    "\n",
    "df[\"outlier\"] = (abs(df[\"z_L\"]) > 5) | (abs(df[\"z_A\"]) > 5) | (abs(df[\"z_P\"]) > 5)\n",
    "\n",
    "df_clean = df[~df[\"outlier\"]].copy()\n",
    "df.shape, df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db0bb1-6e7d-462f-a688-1624fbf1b2c9",
   "metadata": {},
   "source": [
    "Se eliminan 15 observaciones (‚âà1.2%), quedando un conjunto final de 1235 registros, \n",
    "adecuado para an√°lisis estad√≠stico robusto sin distorsionar el comportamiento global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ecabd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas iniciales: 1250 | Filas tras limpiar nulos: 1250\n",
      "Experimentos detectados: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peso_g</th>\n",
       "      <th>L_real</th>\n",
       "      <th>A_real</th>\n",
       "      <th>Longitud_px</th>\n",
       "      <th>Anchura_px</th>\n",
       "      <th>L_av</th>\n",
       "      <th>A_av</th>\n",
       "      <th>errL_pct</th>\n",
       "      <th>errA_pct</th>\n",
       "      <th>exp</th>\n",
       "      <th>peso_pred</th>\n",
       "      <th>err_abs_peso</th>\n",
       "      <th>err_rel_peso</th>\n",
       "      <th>errL_mm</th>\n",
       "      <th>errA_mm</th>\n",
       "      <th>errW_g</th>\n",
       "      <th>abs_errL_mm</th>\n",
       "      <th>abs_errA_mm</th>\n",
       "      <th>abs_errW_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.46</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>302</td>\n",
       "      <td>118</td>\n",
       "      <td>33.30</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1</td>\n",
       "      <td>0.483591</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>5.128430</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.023591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.46</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>299</td>\n",
       "      <td>118</td>\n",
       "      <td>32.96</td>\n",
       "      <td>13.01</td>\n",
       "      <td>-0.121212</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>5</td>\n",
       "      <td>0.473780</td>\n",
       "      <td>0.013780</td>\n",
       "      <td>2.995702</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.013780</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.013780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.46</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>299</td>\n",
       "      <td>118</td>\n",
       "      <td>32.96</td>\n",
       "      <td>13.01</td>\n",
       "      <td>-0.121212</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>3</td>\n",
       "      <td>0.473780</td>\n",
       "      <td>0.013780</td>\n",
       "      <td>2.995702</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.013780</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.013780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   peso_g  L_real  A_real  Longitud_px  Anchura_px   L_av   A_av  errL_pct  \\\n",
       "0    0.46      33      13          302         118  33.30  13.01  0.909091   \n",
       "1    0.46      33      13          299         118  32.96  13.01 -0.121212   \n",
       "2    0.46      33      13          299         118  32.96  13.01 -0.121212   \n",
       "\n",
       "   errA_pct  exp  peso_pred  err_abs_peso  err_rel_peso  errL_mm  errA_mm  \\\n",
       "0  0.076923    1   0.483591      0.023591      5.128430     0.30     0.01   \n",
       "1  0.076923    5   0.473780      0.013780      2.995702    -0.04     0.01   \n",
       "2  0.076923    3   0.473780      0.013780      2.995702    -0.04     0.01   \n",
       "\n",
       "     errW_g  abs_errL_mm  abs_errA_mm  abs_errW_g  \n",
       "0  0.023591         0.30         0.01    0.023591  \n",
       "1  0.013780         0.04         0.01    0.013780  \n",
       "2  0.013780         0.04         0.01    0.013780  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Carga y preprocesado\n",
    "# =========================\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "# Validaci√≥n de columnas\n",
    "missing = [c for c in COLS.values() if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas requeridas: {missing}\")\n",
    "\n",
    "# Forzar tipos num√©ricos\n",
    "for c in COLS.values():\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Eliminar filas con datos incompletos\n",
    "n0 = len(df)\n",
    "df = df.dropna(subset=list(COLS.values())).copy()\n",
    "n1 = len(df)\n",
    "\n",
    "# Residuales (visi√≥n - real) y errores absolutos\n",
    "df[\"errL_mm\"] = df[COLS[\"L_av\"]] - df[COLS[\"L_real\"]]\n",
    "df[\"errA_mm\"] = df[COLS[\"A_av\"]] - df[COLS[\"A_real\"]]\n",
    "df[\"errW_g\"]  = df[COLS[\"peso_pred\"]] - df[COLS[\"peso_real\"]]\n",
    "\n",
    "df[\"abs_errL_mm\"] = df[\"errL_mm\"].abs()\n",
    "df[\"abs_errA_mm\"] = df[\"errA_mm\"].abs()\n",
    "df[\"abs_errW_g\"]  = df[\"errW_g\"].abs()\n",
    "\n",
    "print(f\"Filas iniciales: {n0} | Filas tras limpiar nulos: {n1}\")\n",
    "print(\"Experimentos detectados:\", sorted(df[COLS[\"exp\"]].unique()))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b187729-e88e-4856-be63-6c49c770cfd4",
   "metadata": {},
   "source": [
    "El conjunto de datos incluye, para cada individuo, errores relativos asociados a las estimaciones de longitud, anchura y peso (errL_pct, errA_pct, err_rel_peso), as√≠ como errores absolutos expresados en unidades f√≠sicas (abs_errL_mm, abs_errA_mm, abs_errW_g). Los errores absolutos se interpretan directamente como la diferencia entre la medida estimada por visi√≥n artificial y la medida de referencia, y constituyen la base principal para la evaluaci√≥n metrol√≥gica del sistema y para el an√°lisis del impacto de la incertidumbre sobre la clasificaci√≥n por talla.\n",
    "\n",
    "Los errores relativos de longitud y anchura se encuentran ya expresados en porcentaje (%) en el dataset original y pueden presentar valores positivos o negativos, reflejando respectivamente situaciones de sobreestimaci√≥n o subestimaci√≥n de la medida real. Dado que los errores absolutos observados son muy reducidos en t√©rminos de magnitud f√≠sica, los valores relativos pueden ser num√©ricamente peque√±os (por ejemplo, del orden de d√©cimas de porcentaje), lo cual es coherente con un alto nivel de precisi√≥n del sistema de visi√≥n artificial. En consecuencia, estos errores relativos se utilizaron tal y como fueron proporcionados, sin aplicar ning√∫n proceso adicional de normalizaci√≥n o reescalado, preservando as√≠ tanto su magnitud como su signo.\n",
    "\n",
    "Con el fin de evitar interpretaciones ambiguas y garantizar la coherencia del an√°lisis estad√≠stico, se verific√≥ previamente que todas las columnas de error relativo estuvieran expresadas de forma consistente en porcentaje y dentro de rangos razonables, descart√°ndose la presencia de valores expresados como fracci√≥n. Adicionalmente, para aquellos an√°lisis centrados exclusivamente en la magnitud del error y no en su direcci√≥n, se consider√≥ el uso del valor absoluto del error relativo como variable complementaria, manteniendo siempre la versi√≥n firmada para el estudio de posibles sesgos sistem√°ticos.\n",
    "\n",
    "### Normalizaci√≥n de errores relativos\n",
    "\n",
    "El conjunto de datos incluye errores relativos asociados a las estimaciones de longitud, anchura y peso (`errL_pct`, `errA_pct`, `err_rel_peso`). En funci√≥n del origen y del proceso de registro, estos errores pueden encontrarse expresados indistintamente como fracci√≥n o como porcentaje. Con el fin de evitar ambig√ºedades en la interpretaci√≥n y garantizar la coherencia metrol√≥gica del an√°lisis estad√≠stico, se implement√≥ un procedimiento autom√°tico de detecci√≥n de escala, normalizando todos los errores relativos a porcentaje (%) cuando fuese necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287376fe-2483-4910-b113-519165a30f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizaci√≥n aplicada: {'errL_pct': True, 'errA_pct': True, 'err_rel_peso': False}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>errL_pct</th>\n",
       "      <th>errL_pct_norm</th>\n",
       "      <th>errA_pct</th>\n",
       "      <th>errA_pct_norm</th>\n",
       "      <th>err_rel_peso</th>\n",
       "      <th>errW_pct_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>90.909091</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>7.692308</td>\n",
       "      <td>5.128430</td>\n",
       "      <td>5.128430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.121212</td>\n",
       "      <td>-12.121212</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>7.692308</td>\n",
       "      <td>2.995702</td>\n",
       "      <td>2.995702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.121212</td>\n",
       "      <td>-12.121212</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>7.692308</td>\n",
       "      <td>2.995702</td>\n",
       "      <td>2.995702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   errL_pct  errL_pct_norm  errA_pct  errA_pct_norm  err_rel_peso  \\\n",
       "0  0.909091      90.909091  0.076923       7.692308      5.128430   \n",
       "1 -0.121212     -12.121212  0.076923       7.692308      2.995702   \n",
       "2 -0.121212     -12.121212  0.076923       7.692308      2.995702   \n",
       "\n",
       "   errW_pct_norm  \n",
       "0       5.128430  \n",
       "1       2.995702  \n",
       "2       2.995702  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_pct(series: pd.Series):\n",
    "    \"\"\"Normaliza fracci√≥n -> porcentaje si mediana(|x|) < 0.5.\"\"\"\n",
    "    s = series.astype(float).copy()\n",
    "    med = np.nanmedian(np.abs(s.values))\n",
    "    if med < 0.5:\n",
    "        return s * 100.0, True\n",
    "    return s, False\n",
    "\n",
    "df[\"errL_pct_norm\"], convL = normalize_pct(df[COLS[\"errL_pct\"]])\n",
    "df[\"errA_pct_norm\"], convA = normalize_pct(df[COLS[\"errA_pct\"]])\n",
    "df[\"errW_pct_norm\"], convW = normalize_pct(df[COLS[\"err_rel_peso\"]])\n",
    "\n",
    "print(\"Normalizaci√≥n aplicada:\", {\"errL_pct\": convL, \"errA_pct\": convA, \"err_rel_peso\": convW})\n",
    "df[[COLS[\"errL_pct\"], \"errL_pct_norm\", COLS[\"errA_pct\"], \"errA_pct_norm\", COLS[\"err_rel_peso\"], \"errW_pct_norm\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144bc399",
   "metadata": {},
   "source": [
    "## Variables disponibles\n",
    "\n",
    "El dataset contiene:\n",
    "\n",
    "- `Peso (g)` : peso real (variable objetivo para validaci√≥n)\n",
    "- `Longitud_REAL (mm)`, `Anchura_REAL (mm)` : medidas reales (referencia)\n",
    "- `Longitud_AV (mm)`, `Anchura_AV (mm)` : medidas obtenidas por visi√≥n artificial (AV)\n",
    "- `Error_longitud_% (AV-REAL)`, `Error_anchura_% (AV-REAL)` : errores relativos metrol√≥gicos ya calculados\n",
    "- `Longitud_px`, `Anchura_px` : medidas en p√≠xeles (no utilizadas aqu√≠ para el modelo; el modelo opera en mm)\n",
    "- `Experimento` : identificador construido (E1..E5)\n",
    "\n",
    "### Enfoque\n",
    "1. Ajuste del modelo con medidas **REAL** (para estimaci√≥n robusta de par√°metros).\n",
    "2. Inferencia operativa usando dimensiones **AV**.\n",
    "3. Evaluaci√≥n del error relativo del peso inferido frente al peso real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46bfd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preparaci√≥n: comprobaciones y columnas logar√≠tmicas ---\n",
    "COL_W = \"Peso (g)\"\n",
    "L_real = \"Longitud_REAL (mm)\"\n",
    "A_real = \"Anchura_REAL (mm)\"\n",
    "L_av = \"Longitud_AV (mm)\"\n",
    "A_av = \"Anchura_AV (mm)\"\n",
    "\n",
    "# Verificar que todas las filas son v√°lidas para log (positivas y no nulas)\n",
    "mask = (\n",
    "    df[[COL_W, L_real, A_real, L_av, A_av]].notna().all(axis=1) &\n",
    "    (df[[COL_W, L_real, A_real, L_av, A_av]] > 0).all(axis=1)\n",
    ")\n",
    "assert mask.all(), \"Hay filas no v√°lidas para log; revisa el dataset.\"\n",
    "\n",
    "# Logs (para modelo alom√©trico)\n",
    "df[\"_log_w\"]   = np.log(df[COL_W])\n",
    "df[\"_log_Lr\"]  = np.log(df[L_real])\n",
    "df[\"_log_Ar\"]  = np.log(df[A_real])\n",
    "df[\"_log_Lav\"] = np.log(df[L_av])\n",
    "df[\"_log_Aav\"] = np.log(df[A_av])\n",
    "\n",
    "df[[COL_W, L_real, A_real, L_av, A_av, \"Experimento\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0317c",
   "metadata": {},
   "source": [
    "## Modelo alom√©trico (log‚Äìlog) y correcci√≥n de retransformaci√≥n\n",
    "\n",
    "Se ajusta un modelo lineal en el espacio logar√≠tmico:\n",
    "\n",
    "\\[\n",
    "\\log(P) = b_0 + b_1 \\log(L) + b_2 \\log(A) + \\varepsilon\n",
    "\\]\n",
    "\n",
    "donde:\n",
    "\n",
    "- \\(P\\) es el peso (g),\n",
    "- \\(L\\) longitud (mm),\n",
    "- \\(A\\) anchura (mm).\n",
    "\n",
    "La predicci√≥n en espacio real se obtiene como:\n",
    "\n",
    "\\[\n",
    "\\hat{P} = \\exp(\\hat{y})\n",
    "\\]\n",
    "\n",
    "y, para reducir el sesgo por retransformaci√≥n, se aplica una correcci√≥n multiplicativa tipo **smearing**:\n",
    "\n",
    "\\[\n",
    "\\hat{P}_{corr} = \\exp(\\hat{y}) \\cdot S, \\quad S = \\frac{1}{n}\\sum_i \\exp(\\hat{\\varepsilon}_i)\n",
    "\\]\n",
    "\n",
    "Este enfoque es especialmente √∫til en producci√≥n porque:\n",
    "- ofrece **interpretabilidad** y **estabilidad**,\n",
    "- es **eficiente computacionalmente** (Edge),\n",
    "- facilita auditor√≠a y recalibraciones controladas si fuese necesario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba91f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ajuste del modelo con dimensiones REAL (global) ---\n",
    "Xr = sm.add_constant(df[[\"_log_Lr\", \"_log_Ar\"]])\n",
    "y = df[\"_log_w\"]\n",
    "\n",
    "model = sm.OLS(y, Xr).fit()\n",
    "print(model.summary())\n",
    "\n",
    "b0 = float(model.params[\"const\"])\n",
    "b1 = float(model.params[\"_log_Lr\"])\n",
    "b2 = float(model.params[\"_log_Ar\"])\n",
    "\n",
    "smearing = float(np.mean(np.exp(model.resid)))\n",
    "\n",
    "k = float(np.exp(b0))\n",
    "k_corr = k * smearing\n",
    "\n",
    "print(\"\\nPar√°metros:\")\n",
    "print(f\"b0={b0:.6f}, b1={b1:.6f}, b2={b2:.6f}\")\n",
    "print(f\"Smearing S={smearing:.6f}\")\n",
    "print(\"\\nF√≥rmula operativa (smearing incluido):\")\n",
    "print(f\"Peso(g) = {k_corr:.12f} * L^{b1:.6f} * A^{b2:.6f}   (L,A en mm)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99178b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inferencia de peso usando dimensiones REAL y AV ---\n",
    "def pred_weight_from_logs(logL, logA):\n",
    "    return np.exp(b0 + b1 * logL + b2 * logA)\n",
    "\n",
    "# Predicci√≥n (naive) y corregida (smearing)\n",
    "df[\"Peso_inferido_REAL\"] = pred_weight_from_logs(df[\"_log_Lr\"], df[\"_log_Ar\"])\n",
    "df[\"Peso_inferido_REAL_smear\"] = df[\"Peso_inferido_REAL\"] * smearing\n",
    "\n",
    "df[\"Peso_inferido_AV\"] = pred_weight_from_logs(df[\"_log_Lav\"], df[\"_log_Aav\"])\n",
    "df[\"Peso_inferido_AV_smear\"] = df[\"Peso_inferido_AV\"] * smearing\n",
    "\n",
    "# Error relativo (%)\n",
    "df[\"Error_rel_peso_REAL_pct\"] = (np.abs(df[\"Peso_inferido_REAL_smear\"] - df[COL_W]) / df[COL_W]) * 100\n",
    "df[\"Error_rel_peso_AV_pct\"]   = (np.abs(df[\"Peso_inferido_AV_smear\"] - df[COL_W]) / df[COL_W]) * 100\n",
    "\n",
    "df[[COL_W, \"Peso_inferido_AV_smear\", \"Error_rel_peso_AV_pct\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9642ac",
   "metadata": {},
   "source": [
    "## Tablas de resultados (estad√≠sticos y coberturas)\n",
    "\n",
    "Se generan tablas para:\n",
    "- estad√≠sticos de peso real,\n",
    "- error metrol√≥gico de longitud/anchura (AV-REAL),\n",
    "- error del peso inferido con AV,\n",
    "- coberturas por umbrales (‚â§5%, ‚â§10%, ...),\n",
    "- resultados por experimento (repetibilidad inter-d√≠a),\n",
    "- resultados por rangos de peso (robustez por tama√±o).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110213a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc(s: pd.Series) -> pd.Series:\n",
    "    return pd.Series({\n",
    "        \"N\": int(s.count()),\n",
    "        \"Media\": float(s.mean()),\n",
    "        \"Mediana\": float(s.median()),\n",
    "        \"Desv.T√≠p.\": float(s.std(ddof=1)),\n",
    "        \"P5\": float(s.quantile(0.05)),\n",
    "        \"P95\": float(s.quantile(0.95)),\n",
    "        \"M√≠n\": float(s.min()),\n",
    "        \"M√°x\": float(s.max()),\n",
    "    })\n",
    "\n",
    "tab_peso = desc(df[COL_W]).to_frame(\"Peso (g)\")\n",
    "tab_errL = desc(df[\"Error_longitud_% (AV-REAL)\"]).to_frame(\"Error_longitud_% (AV-REAL)\")\n",
    "tab_errA = desc(df[\"Error_anchura_% (AV-REAL)\"]).to_frame(\"Error_anchura_% (AV-REAL)\")\n",
    "tab_errW_AV = desc(df[\"Error_rel_peso_AV_pct\"]).to_frame(\"Error_rel_peso_AV_pct\")\n",
    "\n",
    "tab_peso.round(3), tab_errW_AV.round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cobertura acumulada por umbrales de error relativo (AV)\n",
    "thresholds = [5, 10, 15, 20]\n",
    "tab_cov = pd.DataFrame([{\n",
    "    \"Umbral (%)\": t,\n",
    "    \"% individuos (<=umbral)\": float((df[\"Error_rel_peso_AV_pct\"] <= t).mean() * 100),\n",
    "    \"N (<=umbral)\": int((df[\"Error_rel_peso_AV_pct\"] <= t).sum()),\n",
    "} for t in thresholds])\n",
    "\n",
    "tab_cov.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed67a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen por experimento (d√≠a)\n",
    "per = df.groupby(\"Experimento\")\n",
    "\n",
    "tab_per_metrology = per.agg({\n",
    "    \"Error_longitud_% (AV-REAL)\": [\"mean\", \"median\", lambda s: s.quantile(0.95)],\n",
    "    \"Error_anchura_% (AV-REAL)\": [\"mean\", \"median\", lambda s: s.quantile(0.95)],\n",
    "})\n",
    "tab_per_metrology.columns = [\"L_mean\", \"L_med\", \"L_P95\", \"A_mean\", \"A_med\", \"A_P95\"]\n",
    "\n",
    "tab_per_weight = per[\"Error_rel_peso_AV_pct\"].agg([\"count\", \"mean\", \"median\", lambda s: s.quantile(0.95), \"max\"])\n",
    "tab_per_weight.columns = [\"N\", \"Media_%\", \"Mediana_%\", \"P95_%\", \"Max_%\"]\n",
    "\n",
    "tab_per_metrology.round(4), tab_per_weight.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error por rangos de peso real (robustez por tama√±o)\n",
    "bins = [-np.inf, 2, 5, 10, np.inf]\n",
    "labels = [\"<2 g\", \"2‚Äì5 g\", \"5‚Äì10 g\", \">=10 g\"]\n",
    "\n",
    "df[\"Rango_peso\"] = pd.cut(df[COL_W], bins=bins, labels=labels)\n",
    "\n",
    "grp = df.groupby(\"Rango_peso\", observed=True)[\"Error_rel_peso_AV_pct\"]\n",
    "tab_bins = pd.DataFrame({\n",
    "    \"N\": grp.count().astype(int),\n",
    "    \"Media_%\": grp.mean(),\n",
    "    \"Mediana_%\": grp.median(),\n",
    "    \"P95_%\": grp.quantile(0.95),\n",
    "    \"Max_%\": grp.max(),\n",
    "}).round(2)\n",
    "\n",
    "tab_bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4564a",
   "metadata": {},
   "source": [
    "## Figuras (generaci√≥n reproducible)\n",
    "\n",
    "Se exportan las figuras a una carpeta local (`jbook_assets_exp`) para poder referenciarlas desde Jupyter-Book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generaci√≥n de figuras ---\n",
    "out_dir = \"jbook_assets_exp\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Figura 4-1: distribuci√≥n errores dimensionales\n",
    "plt.figure()\n",
    "plt.hist(df[\"Error_longitud_% (AV-REAL)\"], bins=40, alpha=0.7, label=\"Error longitud (%)\")\n",
    "plt.hist(df[\"Error_anchura_% (AV-REAL)\"], bins=40, alpha=0.7, label=\"Error anchura (%)\")\n",
    "plt.xlabel(\"Error relativo (%)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Distribuci√≥n del error relativo en medidas AV vs REAL (5 experimentos)\")\n",
    "plt.legend()\n",
    "fig1 = os.path.join(out_dir, \"fig_4_1_error_dimensiones_5exp.png\")\n",
    "plt.tight_layout(); plt.savefig(fig1, dpi=200); plt.close()\n",
    "\n",
    "# Figura 4-2: histograma error de peso AV\n",
    "plt.figure()\n",
    "plt.hist(df[\"Error_rel_peso_AV_pct\"], bins=50)\n",
    "plt.xlabel(\"Error relativo del peso inferido (%)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Histograma del error relativo del peso inferido (dimensiones AV)\")\n",
    "fig2 = os.path.join(out_dir, \"fig_4_2_hist_error_peso_5exp.png\")\n",
    "plt.tight_layout(); plt.savefig(fig2, dpi=200); plt.close()\n",
    "\n",
    "# Figura 4-3: boxplot error por experimento\n",
    "plt.figure()\n",
    "data_box = [df.loc[df[\"Experimento\"] == i, \"Error_rel_peso_AV_pct\"].values for i in range(1, 6)]\n",
    "plt.boxplot(data_box, labels=[f\"E{i}\" for i in range(1, 6)], showfliers=True)\n",
    "plt.xlabel(\"Experimento (d√≠a)\")\n",
    "plt.ylabel(\"Error relativo del peso inferido (%)\")\n",
    "plt.title(\"Variabilidad inter-experimento del error de peso (AV)\")\n",
    "fig3 = os.path.join(out_dir, \"fig_4_3_boxplot_error_peso_por_experimento.png\")\n",
    "plt.tight_layout(); plt.savefig(fig3, dpi=200); plt.close()\n",
    "\n",
    "# Figura 4-4: boxplot por rango de peso\n",
    "plt.figure()\n",
    "data_box = [df.loc[df[\"Rango_peso\"] == lbl, \"Error_rel_peso_AV_pct\"].values for lbl in [\"<2 g\", \"2‚Äì5 g\", \"5‚Äì10 g\", \">=10 g\"]]\n",
    "plt.boxplot(data_box, labels=[\"<2 g\", \"2‚Äì5 g\", \"5‚Äì10 g\", \">=10 g\"], showfliers=True)\n",
    "plt.xlabel(\"Rango de peso real\")\n",
    "plt.ylabel(\"Error relativo del peso inferido (%)\")\n",
    "plt.title(\"Error de peso inferido (AV) por rangos de peso real\")\n",
    "fig4 = os.path.join(out_dir, \"fig_4_4_boxplot_error_por_rango_peso.png\")\n",
    "plt.tight_layout(); plt.savefig(fig4, dpi=200); plt.close()\n",
    "\n",
    "# Figura 4-5: scatter peso inferido vs real por experimento\n",
    "plt.figure()\n",
    "for i in range(1, 6):\n",
    "    sub = df[df[\"Experimento\"] == i]\n",
    "    plt.scatter(sub[COL_W], sub[\"Peso_inferido_AV_smear\"], s=10, label=f\"E{i}\", alpha=0.7)\n",
    "mx = float(np.nanmax([df[COL_W].max(), df[\"Peso_inferido_AV_smear\"].max()]))\n",
    "plt.plot([0, mx], [0, mx])\n",
    "plt.xlabel(\"Peso real (g)\")\n",
    "plt.ylabel(\"Peso inferido (g)\")\n",
    "plt.title(\"Peso inferido vs real por experimento (AV)\")\n",
    "plt.legend()\n",
    "fig5 = os.path.join(out_dir, \"fig_4_5_scatter_peso_real_vs_inferido_por_experimento.png\")\n",
    "plt.tight_layout(); plt.savefig(fig5, dpi=200); plt.close()\n",
    "\n",
    "# Figura 4-6: evoluci√≥n media y P95 por experimento\n",
    "plt.figure()\n",
    "x = np.arange(1, 6)\n",
    "plt.plot(x, tab_per_weight[\"Media_%\"].values, marker=\"o\", label=\"Media (%)\")\n",
    "plt.plot(x, tab_per_weight[\"P95_%\"].values, marker=\"o\", label=\"P95 (%)\")\n",
    "plt.xlabel(\"Experimento (d√≠a)\")\n",
    "plt.ylabel(\"Error relativo (%)\")\n",
    "plt.title(\"Evoluci√≥n de error de peso (media y P95) por experimento\")\n",
    "plt.legend()\n",
    "fig6 = os.path.join(out_dir, \"fig_4_6_media_p95_por_experimento.png\")\n",
    "plt.tight_layout(); plt.savefig(fig6, dpi=200); plt.close()\n",
    "\n",
    "[fig1, fig2, fig3, fig4, fig5, fig6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81992a39",
   "metadata": {},
   "source": [
    "## Discusi√≥n t√©cnico-operativa (orientaci√≥n CDTI)\n",
    "\n",
    "### Lecturas principales (resumen)\n",
    "\n",
    "- **Metrolog√≠a (AV vs REAL):** los errores relativos de longitud y anchura permanecen contenidos y consistentes a lo largo de 5 d√≠as, lo que aporta evidencia de **repetibilidad del pipeline de visi√≥n** con configuraci√≥n constante.\n",
    "- **Error de peso inferido (AV):** el error relativo presenta estabilidad central compatible con clasificaci√≥n por tama√±os. La dispersi√≥n en el extremo bajo (<2 g) es esperable por naturaleza geom√©trica y biol√≥gica.\n",
    "- **Repetibilidad inter-d√≠a:** la comparaci√≥n por experimento (media/mediana/P95) permite argumentar **robustez temporal sin recalibraci√≥n**, aspecto especialmente relevante para justificar TRL en memoria CDTI.\n",
    "- **Criterios de aceptaci√≥n recomendados para clasificaci√≥n:** mediana ‚â§10%, cobertura ‚â§10% ‚â•55‚Äì65%, P95 ‚â§25%, apoyado por QA para casos at√≠picos.\n",
    "\n",
    "### TRL (s√≠ntesis)\n",
    "Con la evidencia multi-d√≠a y el modelo √∫nico sin recalibraci√≥n, la tecnolog√≠a se sit√∫a en **TRL 6‚Äì7**, dependiendo del grado de equivalencia del banco de ensayo con la l√≠nea real (cadencia, manipulaci√≥n, iluminaci√≥n, etc.).\n",
    "\n",
    "### Recomendaci√≥n para reforzar TRL 7‚Üí8\n",
    "- A√±adir trazas operativas del entorno (par√°metros de captura, control QA, incidencias)\n",
    "- Pilotos adicionales bajo variabilidad controlada (lote, densidad, iluminaci√≥n)\n",
    "- Integraci√≥n final con criterio de clasificaci√≥n (umbrales por categor√≠a) y medici√≥n de tasa de recaptura/‚Äúerrores‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) Exportar un Excel enriquecido con columnas calculadas para auditor√≠a/QA\n",
    "out_xlsx = \"Datos_experimentos_lenguado_enriquecido.xlsx\"\n",
    "df.to_excel(out_xlsx, index=False)\n",
    "out_xlsx\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
